{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nHeidi Jiang\\n05/13/2025\\n\\nThis program analyzes three forums regarding charity bike rides by calculating\\nthe polarity scores and applying LDA analysis.\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Heidi Jiang\n",
    "charity_ride.ipynb\n",
    "05/13/2025\n",
    "\n",
    "This program analyzes three forums regarding charity bike rides by calculating\n",
    "the polarity scores and applying LDA analysis.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/hyjiang/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# imports\n",
    "from urllib.request import Request, urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from textblob import TextBlob\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "HEADERS = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "URL_50_PLUS = \"https://www.bikeforums.net/fifty-plus-50/1193362-charity-rides.html\"\n",
    "URL_MIN_FUND = \"https://www.bikeforums.net/charity-events/1176088-charity-rides-minimum-fundraising-requirements.html\"\n",
    "URL_IMPACT = \"https://www.cyclingforums.com/threads/have-you-participated-in-charity-rides-before-if-yes-could-you-share-your-experience-and-its-impact-on-yourself-as-well-as-others.488013/\"\n",
    "ID_PATTERN = r'^post_message_'\n",
    "\n",
    "\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "CUSTOM_SW = [\"ride\", \"rides\", \"riders\", \"quoteoriginally\", \"posted\", \"like\",\n",
    "    \"one\", \"dont\", \"youre\", \"sure\", \"back\", \"time\", \"event\", \"events\", \"want\",\n",
    "    \"lets\", \"ms\", \"every\", \"got\", \"get\", \"well\", \"truly\", \"ive\", \"theyre\",\n",
    "    \"also\", \"im\", \"done\", \"impact\", \"experience\", \"charity\"]\n",
    "ALL_SW = STOPWORDS.union(CUSTOM_SW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# web scraping for first URL\n",
    "req_50_plus = Request(URL_50_PLUS, headers=HEADERS)\n",
    "html_50_plus = urlopen(req_50_plus)\n",
    "bs_50_plus = BeautifulSoup(html_50_plus.read(), \"html.parser\")\n",
    "texts_50_plus = bs_50_plus.find_all(\"div\", id=re.compile(r'^post_message_'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# web scraping for second URL\n",
    "req_min_fund = Request(URL_MIN_FUND, headers=HEADERS)\n",
    "html_min_fund = urlopen(req_min_fund)\n",
    "bs_min_fund = BeautifulSoup(html_min_fund.read(), \"html.parser\")\n",
    "texts_min_fund = bs_min_fund.find_all(\"div\", id=re.compile(r'^post_message_'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# web scraping for third URL\n",
    "req_impact = Request(URL_IMPACT, headers=HEADERS)\n",
    "html_impact = urlopen(req_impact)\n",
    "bs_impact = BeautifulSoup(html_impact.read(), \"html.parser\")\n",
    "texts_impact = bs_impact.find_all(\"div\", {\"class\": \"bbWrapper\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polarity_score(texts):\n",
    "    \"\"\"\n",
    "    Given a list of BeautifulSoup objects, calculate the polarity score\n",
    "    \"\"\"\n",
    "    sentiments = []\n",
    "    for text in texts:\n",
    "        blob = TextBlob(text.get_text())\n",
    "        sentiments.append(blob.sentiment.polarity)\n",
    "    polarity = sum(sentiments) / len(sentiments)\n",
    "    return polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(texts):\n",
    "    \"\"\"\n",
    "    Given a list of BeautifulSoup objects, clean the text\n",
    "    \"\"\"\n",
    "    cleaned = []\n",
    "    for t in texts:\n",
    "        text = t.get_text(strip=True).lower()\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "        tokens = text.split()\n",
    "        tokens = [token for token in tokens if token not in ALL_SW]\n",
    "        cleaned.append(tokens)\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lda_model(texts, n_topics=1, n_top_words=5):\n",
    "    \"\"\"\n",
    "    Given a list of BeautifulSoup objects, apply LDA analysis\n",
    "    \"\"\"\n",
    "    docs = clean_text(texts)\n",
    "    docs = [' '.join(doc) for doc in docs]\n",
    "    vectorizer = CountVectorizer(max_df=0.95, min_df=2)\n",
    "    dtm = vectorizer.fit_transform(docs)\n",
    "\n",
    "    lda_model = LatentDirichletAllocation(n_components=n_topics)\n",
    "    lda_model.fit(dtm)\n",
    "\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    topics = []\n",
    "    for i, topic in enumerate(lda_model.components_):\n",
    "        top_words_idx = topic.argsort()[:-n_top_words - 1:-1]\n",
    "        top_words = [feature_names[i] for i in top_words_idx]\n",
    "        topics.append(top_words)\n",
    "    return topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bikers over 50\n",
      "Polarity score: 0.12\n",
      "Main topic: food support century club course\n",
      "\n",
      "Charity rides with minimum fundraising requirements\n",
      "Polarity score: 0.16\n",
      "Main topic: minimum fee costs money people\n",
      "\n",
      "Experience and impact of charity rides\n",
      "Polarity score: 0.19\n",
      "Main topic: community personal social change fitness\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# results\n",
    "agg_texts = [texts_50_plus, texts_min_fund, texts_impact]\n",
    "descr = [\"Bikers over 50\",\n",
    "        \"Charity rides with minimum fundraising requirements\",\n",
    "        \"Experience and impact of charity rides\"]\n",
    "\n",
    "for i, texts in enumerate(agg_texts):\n",
    "    print(f\"{descr[i]}\")\n",
    "    pol = polarity_score(texts)\n",
    "    topic = \" \".join(lda_model(texts)[0])\n",
    "    print(f\"Polarity score: {pol:.2f}\")\n",
    "    print(f\"Main topic: {topic}\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "charity_ride",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
